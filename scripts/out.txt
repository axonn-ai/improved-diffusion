[2024-01-08 15:26:58,512] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-08 15:26:58,512] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-08 15:26:59,951] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-08 15:26:59,951] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-08 15:26:59,951] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-01-08 15:26:59,951] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-01-08 15:26:59,953] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=2, master_addr=10.70.19.45, master_port=29500
[2024-01-08 15:26:59,953] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=2, master_addr=10.70.19.45, master_port=29500
[2024-01-08 15:26:59,953] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Logging to /tmp/openai-2024-01-08-15-27-00-961877
initialized deepspeed
creating model and diffusion...
creating data loader...
[2024-01-08 15:27:05,026] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown
[2024-01-08 15:27:05,753] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-01-08 15:27:05,754] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-01-08 15:27:05,754] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-01-08 15:27:05,790] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-01-08 15:27:05,790] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-01-08 15:27:05,790] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-01-08 15:27:05,790] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer
[2024-01-08 15:27:05,845] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning
[2024-01-08 15:27:05,846] [INFO] [utils.py:792:see_memory_usage] MA 0.2 GB         Max_MA 0.2 GB         CA 0.21 GB         Max_CA 0 GB 
[2024-01-08 15:27:05,846] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.39 GB, percent = 5.4%
[2024-01-08 15:27:05,849] [INFO] [stage3.py:127:__init__] Reduce bucket size 10000000
[2024-01-08 15:27:05,849] [INFO] [stage3.py:128:__init__] Prefetch bucket size 10000000
[2024-01-08 15:27:05,903] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-01-08 15:27:05,904] [INFO] [utils.py:792:see_memory_usage] MA 0.2 GB         Max_MA 0.2 GB         CA 0.21 GB         Max_CA 0 GB 
[2024-01-08 15:27:05,904] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.39 GB, percent = 5.4%
Parameter Offload: Total persistent parameters: 1428358 in 323 params
[2024-01-08 15:27:06,174] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-01-08 15:27:06,175] [INFO] [utils.py:792:see_memory_usage] MA 0.1 GB         Max_MA 0.2 GB         CA 0.21 GB         Max_CA 0 GB 
[2024-01-08 15:27:06,175] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.54 GB, percent = 5.7%
[2024-01-08 15:27:06,242] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions
[2024-01-08 15:27:06,243] [INFO] [utils.py:792:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 0.21 GB         Max_CA 0 GB 
[2024-01-08 15:27:06,243] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.55 GB, percent = 5.7%
[2024-01-08 15:27:06,407] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 1
[2024-01-08 15:27:06,408] [INFO] [utils.py:792:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 0.1 GB         Max_CA 0 GB 
[2024-01-08 15:27:06,408] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.55 GB, percent = 5.7%
[2024-01-08 15:27:06,469] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions
[2024-01-08 15:27:06,470] [INFO] [utils.py:792:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 0.1 GB         Max_CA 0 GB 
[2024-01-08 15:27:06,470] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.55 GB, percent = 5.7%
[2024-01-08 15:27:06,531] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions
[2024-01-08 15:27:06,531] [INFO] [utils.py:792:see_memory_usage] MA 0.2 GB         Max_MA 0.2 GB         CA 0.2 GB         Max_CA 0 GB 
[2024-01-08 15:27:06,531] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.55 GB, percent = 5.7%
[2024-01-08 15:27:06,593] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-01-08 15:27:06,593] [INFO] [utils.py:792:see_memory_usage] MA 0.2 GB         Max_MA 0.2 GB         CA 0.2 GB         Max_CA 0 GB 
[2024-01-08 15:27:06,593] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.58 GB, percent = 5.8%
[2024-01-08 15:27:06,680] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-01-08 15:27:06,681] [INFO] [utils.py:792:see_memory_usage] MA 0.39 GB         Max_MA 0.59 GB         CA 0.6 GB         Max_CA 1 GB 
[2024-01-08 15:27:06,681] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.61 GB, percent = 5.8%
[2024-01-08 15:27:06,681] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
[2024-01-08 15:27:06,864] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-01-08 15:27:06,864] [INFO] [utils.py:792:see_memory_usage] MA 0.53 GB         Max_MA 0.54 GB         CA 0.6 GB         Max_CA 1 GB 
[2024-01-08 15:27:06,865] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 3.63 GB, percent = 5.8%
[2024-01-08 15:27:06,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-01-08 15:27:06,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-01-08 15:27:06,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-01-08 15:27:06,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]
[2024-01-08 15:27:06,866] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   amp_params ................... False
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f33000e08b0>
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   communication_data_type ...... None
[2024-01-08 15:27:06,866] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   dump_state ................... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   loss_scale ................... 0
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-01-08 15:27:06,867] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   pld_params ................... False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   steps_per_print .............. 10
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   train_batch_size ............. 128
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  64
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   world_size ................... 2
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=10000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=10000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-01-08 15:27:06,868] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
[2024-01-08 15:27:06,868] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 128, 
    "zero_optimization": {
        "stage": 3, 
        "contiguous_gradients": true, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_prefetch_bucket_size": 1.000000e+07, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "reduce_bucket_size": 1.000000e+07, 
        "sub_group_size": 1.000000e+09
    }
}
training...
THIS IS WHERE LOGGING USUALLY STARTS
Done with 1 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 2 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 3 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 4 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 5 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 6 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 7 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 8 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 9 steps
[2024-01-08 15:30:28,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[9.55e-05], mom=[(0.9, 0.999)]
[2024-01-08 15:30:28,632] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=6.485374838373263, CurrSamplesPerSec=6.5125358854832065, MemAllocated=0.55GB, MaxMemAllocated=0.99GB
THIS IS WHERE LOGGING USUALLY STARTS
Done with 10 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 11 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 12 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 13 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 14 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 15 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 16 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 17 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 18 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 19 steps
[2024-01-08 15:33:45,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[9.05e-05], mom=[(0.9, 0.999)]
[2024-01-08 15:33:45,874] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=6.487665509084449, CurrSamplesPerSec=6.4862024812657415, MemAllocated=0.55GB, MaxMemAllocated=0.99GB
THIS IS WHERE LOGGING USUALLY STARTS
Done with 20 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 21 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 22 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 23 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 24 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 25 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 26 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 27 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 28 steps
THIS IS WHERE LOGGING USUALLY STARTS
Done with 29 steps
[2024-01-08 15:37:03,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[8.55e-05], mom=[(0.9, 0.999)]
[2024-01-08 15:37:03,299] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=6.486184205942969, CurrSamplesPerSec=6.481449410288946, MemAllocated=0.55GB, MaxMemAllocated=0.99GB
