WORLD SIZE: 1
CUDA AVAILABLE: True
Logging to /tmp/openai-2024-01-25-15-34-35-379201
creating model and diffusion...
creating data loader...
training...
----------------------------
| grad_norm     | 2.82     |
| lg_loss_scale | 20       |
| loss          | 1.01     |
| loss_q0       | 1.02     |
| loss_q1       | 1.01     |
| loss_q2       | 1        |
| loss_q3       | 1.02     |
| mse           | 0.998    |
| mse_q0        | 0.998    |
| mse_q1        | 1        |
| mse_q2        | 0.995    |
| mse_q3        | 0.996    |
| samples       | 128      |
| step          | 0        |
| vb            | 0.0153   |
| vb_q0         | 0.0232   |
| vb_q1         | 0.00514  |
| vb_q2         | 0.00504  |
| vb_q3         | 0.0253   |
----------------------------
----------------------------
| grad_norm     | 3.02     |
| lg_loss_scale | 20       |
| loss          | 0.928    |
| loss_q0       | 0.955    |
| loss_q1       | 0.91     |
| loss_q2       | 0.905    |
| loss_q3       | 0.941    |
| mse           | 0.911    |
| mse_q0        | 0.934    |
| mse_q1        | 0.905    |
| mse_q2        | 0.9      |
| mse_q3        | 0.905    |
| samples       | 1.41e+03 |
| step          | 10       |
| vb            | 0.0165   |
| vb_q0         | 0.0207   |
| vb_q1         | 0.00454  |
| vb_q2         | 0.00457  |
| vb_q3         | 0.036    |
----------------------------
----------------------------
| grad_norm     | 2.85     |
| lg_loss_scale | 20       |
| loss          | 0.747    |
| loss_q0       | 0.799    |
| loss_q1       | 0.726    |
| loss_q2       | 0.72     |
| loss_q3       | 0.747    |
| mse           | 0.734    |
| mse_q0        | 0.78     |
| mse_q1        | 0.722    |
| mse_q2        | 0.716    |
| mse_q3        | 0.719    |
| samples       | 2.69e+03 |
| step          | 20       |
| vb            | 0.0135   |
| vb_q0         | 0.0186   |
| vb_q1         | 0.00361  |
| vb_q2         | 0.00362  |
| vb_q3         | 0.0287   |
----------------------------
----------------------------
| grad_norm     | 2.49     |
| lg_loss_scale | 20       |
| loss          | 0.601    |
| loss_q0       | 0.673    |
| loss_q1       | 0.573    |
| loss_q2       | 0.56     |
| loss_q3       | 0.595    |
| mse           | 0.586    |
| mse_q0        | 0.655    |
| mse_q1        | 0.571    |
| mse_q2        | 0.557    |
| mse_q3        | 0.559    |
| samples       | 3.97e+03 |
| step          | 30       |
| vb            | 0.0148   |
| vb_q0         | 0.0179   |
| vb_q1         | 0.00286  |
| vb_q2         | 0.00283  |
| vb_q3         | 0.0351   |
----------------------------
----------------------------
| grad_norm     | 2.16     |
| lg_loss_scale | 20       |
| loss          | 0.479    |
| loss_q0       | 0.584    |
| loss_q1       | 0.447    |
| loss_q2       | 0.434    |
| loss_q3       | 0.448    |
| mse           | 0.466    |
| mse_q0        | 0.553    |
| mse_q1        | 0.445    |
| mse_q2        | 0.432    |
| mse_q3        | 0.43     |
| samples       | 5.25e+03 |
| step          | 40       |
| vb            | 0.013    |
| vb_q0         | 0.0308   |
| vb_q1         | 0.00227  |
| vb_q2         | 0.00217  |
| vb_q3         | 0.0178   |
----------------------------
----------------------------
| grad_norm     | 1.87     |
| lg_loss_scale | 20.1     |
| loss          | 0.378    |
| loss_q0       | 0.477    |
| loss_q1       | 0.35     |
| loss_q2       | 0.336    |
| loss_q3       | 0.34     |
| mse           | 0.369    |
| mse_q0        | 0.457    |
| mse_q1        | 0.348    |
| mse_q2        | 0.335    |
| mse_q3        | 0.328    |
| samples       | 6.53e+03 |
| step          | 50       |
| vb            | 0.00916  |
| vb_q0         | 0.0198   |
| vb_q1         | 0.00175  |
| vb_q2         | 0.0017   |
| vb_q3         | 0.0116   |
----------------------------
----------------------------
| grad_norm     | 1.61     |
| lg_loss_scale | 20.1     |
| loss          | 0.306    |
| loss_q0       | 0.418    |
| loss_q1       | 0.279    |
| loss_q2       | 0.259    |
| loss_q3       | 0.263    |
| mse           | 0.297    |
| mse_q0        | 0.394    |
| mse_q1        | 0.277    |
| mse_q2        | 0.258    |
| mse_q3        | 0.252    |
| samples       | 7.81e+03 |
| step          | 60       |
| vb            | 0.00932  |
| vb_q0         | 0.0232   |
| vb_q1         | 0.00141  |
| vb_q2         | 0.0013   |
| vb_q3         | 0.0115   |
----------------------------
----------------------------
| grad_norm     | 1.39     |
| lg_loss_scale | 20.1     |
| loss          | 0.252    |
| loss_q0       | 0.381    |
| loss_q1       | 0.222    |
| loss_q2       | 0.203    |
| loss_q3       | 0.208    |
| mse           | 0.242    |
| mse_q0        | 0.359    |
| mse_q1        | 0.221    |
| mse_q2        | 0.202    |
| mse_q3        | 0.193    |
| samples       | 9.09e+03 |
| step          | 70       |
| vb            | 0.00965  |
| vb_q0         | 0.0223   |
| vb_q1         | 0.00111  |
| vb_q2         | 0.00103  |
| vb_q3         | 0.015    |
----------------------------
----------------------------
| grad_norm     | 1.19     |
| lg_loss_scale | 20.1     |
| loss          | 0.212    |
| loss_q0       | 0.357    |
| loss_q1       | 0.185    |
| loss_q2       | 0.158    |
| loss_q3       | 0.157    |
| mse           | 0.205    |
| mse_q0        | 0.332    |
| mse_q1        | 0.184    |
| mse_q2        | 0.157    |
| mse_q3        | 0.151    |
| samples       | 1.04e+04 |
| step          | 80       |
| vb            | 0.00776  |
| vb_q0         | 0.0249   |
| vb_q1         | 0.000927 |
| vb_q2         | 0.000804 |
| vb_q3         | 0.00555  |
----------------------------
----------------------------
| grad_norm     | 1.05     |
| lg_loss_scale | 20.1     |
| loss          | 0.173    |
| loss_q0       | 0.291    |
| loss_q1       | 0.156    |
| loss_q2       | 0.129    |
| loss_q3       | 0.128    |
| mse           | 0.168    |
| mse_q0        | 0.282    |
| mse_q1        | 0.155    |
| mse_q2        | 0.128    |
| mse_q3        | 0.119    |
| samples       | 1.16e+04 |
| step          | 90       |
| vb            | 0.00476  |
| vb_q0         | 0.00875  |
| vb_q1         | 0.000786 |
| vb_q2         | 0.000648 |
| vb_q3         | 0.00885  |
----------------------------
----------------------------
| grad_norm     | 0.908    |
| lg_loss_scale | 20.1     |
| loss          | 0.384    |
| loss_q0       | 0.29     |
| loss_q1       | 0.133    |
| loss_q2       | 0.107    |
| loss_q3       | 0.989    |
| mse           | 0.152    |
| mse_q0        | 0.278    |
| mse_q1        | 0.133    |
| mse_q2        | 0.106    |
| mse_q3        | 0.095    |
| samples       | 1.29e+04 |
| step          | 100      |
| vb            | 0.232    |
| vb_q0         | 0.0123   |
| vb_q1         | 0.000664 |
| vb_q2         | 0.000536 |
| vb_q3         | 0.894    |
----------------------------
----------------------------
| grad_norm     | 0.803    |
| lg_loss_scale | 20.1     |
| loss          | 0.137    |
| loss_q0       | 0.281    |
| loss_q1       | 0.119    |
| loss_q2       | 0.09     |
| loss_q3       | 0.0796   |
| mse           | 0.132    |
| mse_q0        | 0.263    |
| mse_q1        | 0.118    |
| mse_q2        | 0.0895   |
| mse_q3        | 0.0776   |
| samples       | 1.42e+04 |
| step          | 110      |
| vb            | 0.00486  |
| vb_q0         | 0.018    |
| vb_q1         | 0.000597 |
| vb_q2         | 0.000453 |
| vb_q3         | 0.00207  |
----------------------------
----------------------------
| grad_norm     | 0.719    |
| lg_loss_scale | 20.1     |
| loss          | 0.235    |
| loss_q0       | 0.273    |
| loss_q1       | 0.109    |
| loss_q2       | 0.0796   |
| loss_q3       | 0.442    |
| mse           | 0.123    |
| mse_q0        | 0.256    |
| mse_q1        | 0.108    |
| mse_q2        | 0.0792   |
| mse_q3        | 0.0648   |
| samples       | 1.55e+04 |
| step          | 120      |
| vb            | 0.112    |
| vb_q0         | 0.0171   |
| vb_q1         | 0.000556 |
| vb_q2         | 0.000397 |
| vb_q3         | 0.377    |
----------------------------
----------------------------
| grad_norm     | 0.669    |
| lg_loss_scale | 20.1     |
| loss          | 0.122    |
| loss_q0       | 0.258    |
| loss_q1       | 0.102    |
| loss_q2       | 0.0697   |
| loss_q3       | 0.0573   |
| mse           | 0.119    |
| mse_q0        | 0.249    |
| mse_q1        | 0.101    |
| mse_q2        | 0.0693   |
| mse_q3        | 0.0554   |
| samples       | 1.68e+04 |
| step          | 130      |
| vb            | 0.00303  |
| vb_q0         | 0.00941  |
| vb_q1         | 0.000517 |
| vb_q2         | 0.000348 |
| vb_q3         | 0.00183  |
----------------------------
----------------------------
| grad_norm     | 0.619    |
| lg_loss_scale | 20.1     |
| loss          | 0.224    |
| loss_q0       | 0.266    |
| loss_q1       | 0.0942   |
| loss_q2       | 0.0629   |
| loss_q3       | 0.45     |
| mse           | 0.116    |
| mse_q0        | 0.251    |
| mse_q1        | 0.0937   |
| mse_q2        | 0.0626   |
| mse_q3        | 0.0494   |
| samples       | 1.8e+04  |
| step          | 140      |
| vb            | 0.108    |
| vb_q0         | 0.0145   |
| vb_q1         | 0.000478 |
| vb_q2         | 0.000318 |
| vb_q3         | 0.4      |
----------------------------
----------------------------
| grad_norm     | 0.551    |
| lg_loss_scale | 20.2     |
| loss          | 0.204    |
| loss_q0       | 0.275    |
| loss_q1       | 0.0899   |
| loss_q2       | 0.0593   |
| loss_q3       | 0.378    |
| mse           | 0.109    |
| mse_q0        | 0.247    |
| mse_q1        | 0.0895   |
| mse_q2        | 0.059    |
| mse_q3        | 0.0447   |
| samples       | 1.93e+04 |
| step          | 150      |
| vb            | 0.0947   |
| vb_q0         | 0.0276   |
| vb_q1         | 0.000458 |
| vb_q2         | 0.000297 |
| vb_q3         | 0.333    |
----------------------------
----------------------------
| grad_norm     | 0.522    |
| lg_loss_scale | 20.2     |
| loss          | 0.209    |
| loss_q0       | 0.265    |
| loss_q1       | 0.0851   |
| loss_q2       | 0.0558   |
| loss_q3       | 0.44     |
| mse           | 0.105    |
| mse_q0        | 0.246    |
| mse_q1        | 0.0847   |
| mse_q2        | 0.0555   |
| mse_q3        | 0.0413   |
| samples       | 2.06e+04 |
| step          | 160      |
| vb            | 0.104    |
| vb_q0         | 0.0191   |
| vb_q1         | 0.000428 |
| vb_q2         | 0.000281 |
| vb_q3         | 0.399    |
----------------------------
----------------------------
| grad_norm     | 0.484    |
| lg_loss_scale | 20.2     |
| loss          | 0.109    |
| loss_q0       | 0.253    |
| loss_q1       | 0.0829   |
| loss_q2       | 0.0517   |
| loss_q3       | 0.0394   |
| mse           | 0.102    |
| mse_q0        | 0.229    |
| mse_q1        | 0.0825   |
| mse_q2        | 0.0514   |
| mse_q3        | 0.038    |
| samples       | 2.19e+04 |
| step          | 170      |
| vb            | 0.00673  |
| vb_q0         | 0.024    |
| vb_q1         | 0.000422 |
| vb_q2         | 0.000259 |
| vb_q3         | 0.00141  |
----------------------------
----------------------------
| grad_norm     | 0.473    |
| lg_loss_scale | 20.2     |
| loss          | 0.102    |
| loss_q0       | 0.24     |
| loss_q1       | 0.0829   |
| loss_q2       | 0.0512   |
| loss_q3       | 0.0371   |
| mse           | 0.0988   |
| mse_q0        | 0.228    |
| mse_q1        | 0.0825   |
| mse_q2        | 0.0509   |
| mse_q3        | 0.0356   |
| samples       | 2.32e+04 |
| step          | 180      |
| vb            | 0.00334  |
| vb_q0         | 0.0113   |
| vb_q1         | 0.000425 |
| vb_q2         | 0.000255 |
| vb_q3         | 0.00142  |
----------------------------
----------------------------
| grad_norm     | 0.45     |
| lg_loss_scale | 20.2     |
| loss          | 0.106    |
| loss_q0       | 0.268    |
| loss_q1       | 0.0829   |
| loss_q2       | 0.05     |
| loss_q3       | 0.0366   |
| mse           | 0.1      |
| mse_q0        | 0.244    |
| mse_q1        | 0.0825   |
| mse_q2        | 0.0498   |
| mse_q3        | 0.0353   |
| samples       | 2.44e+04 |
| step          | 190      |
| vb            | 0.00608  |
| vb_q0         | 0.0236   |
| vb_q1         | 0.000428 |
| vb_q2         | 0.000249 |
| vb_q3         | 0.00135  |
----------------------------
